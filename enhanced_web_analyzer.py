# -*- coding: utf-8 -*-

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import pandas as pd
from playwright.sync_api import sync_playwright, Error as PlaywrightError
from bs4 import BeautifulSoup, element as bs4_element
import json
import time
import re
import os
import uuid
from urllib.parse import urljoin, urlparse
from collections import defaultdict
from datetime import datetime

app = Flask(__name__)
CORS(app)  # Ù„ØªÙ…ÙƒÙŠÙ† CORS Ù„Ù„ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©

class WebPageAnalyzer:
    def __init__(self):
        # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© ÙˆØ§Ù„ÙˆØ³Ø§Ø¦Ø· ÙˆØ§Ù„Ù†ØµÙˆØµ
        self.MEDIA_TAGS = {'img', 'video', 'audio', 'source', 'picture', 'iframe', 'embed', 'object'}
        self.INTERACTIVE_TAGS = {'a', 'button', 'input', 'select', 'textarea', 'form', 'details', 'summary'}
        self.TEXT_TAGS = {'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'blockquote', 'span', 'div', 'article', 'section'}
        self.NAVIGATION_TAGS = {'nav', 'menu', 'menuitem'}
        self.LIST_TAGS = {'ul', 'ol', 'dl'}
        
        # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
        self.INPUT_TYPES = {
            'text': 'Ù†Øµ',
            'email': 'Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ', 
            'password': 'ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±',
            'number': 'Ø±Ù‚Ù…',
            'tel': 'Ù‡Ø§ØªÙ',
            'url': 'Ø±Ø§Ø¨Ø·',
            'search': 'Ø¨Ø­Ø«',
            'date': 'ØªØ§Ø±ÙŠØ®',
            'time': 'ÙˆÙ‚Øª',
            'checkbox': 'Ù…Ø±Ø¨Ø¹ Ø§Ø®ØªÙŠØ§Ø±',
            'radio': 'Ø§Ø®ØªÙŠØ§Ø± ÙˆØ§Ø­Ø¯',
            'file': 'Ù…Ù„Ù',
            'submit': 'Ø¥Ø±Ø³Ø§Ù„',
            'reset': 'Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†',
            'button': 'Ø²Ø±'
        }

    def generate_advanced_selector(self, element: bs4_element.Tag) -> dict:
        """ÙŠÙ†Ø´Ø¦ Ù…ÙØ­Ø¯Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ø¹Ù†ØµØ± Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        selectors = {}
        
        # Ø§Ù„Ù…ÙØ­Ø¯Ø¯ Ø¨Ø§Ù„Ù€ ID (Ø§Ù„Ø£Ù‚ÙˆÙ‰)
        if element.has_attr('id'):
            selectors['id_selector'] = f"#{element['id']}"
            selectors['priority'] = 'high'
        
        # Ø§Ù„Ù…ÙØ­Ø¯Ø¯ Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³
        if element.has_attr('class'):
            classes = [c for c in element['class'] if c]
            if classes:
                selectors['class_selector'] = f"{element.name}.{'.'.join(classes)}"
                selectors['classes'] = classes
        
        # Ø§Ù„Ù…ÙØ­Ø¯Ø¯ Ø¨Ø§Ù„Ø®ØµØ§Ø¦Øµ
        if element.has_attr('name'):
            selectors['name_selector'] = f"{element.name}[name='{element['name']}']"
        
        if element.has_attr('type'):
            selectors['type_selector'] = f"{element.name}[type='{element['type']}']"
        
        # Ø§Ù„Ù…ÙØ­Ø¯Ø¯ Ø§Ù„Ù†Ø³Ø¨ÙŠ (Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù†ØµØ±)
        selectors['tag_selector'] = element.name
        
        # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù‡Ø±Ù…ÙŠ
        path_parts = []
        current = element
        while current and current.name:
            if current.has_attr('id'):
                path_parts.append(f"{current.name}#{current['id']}")
                break
            elif current.has_attr('class'):
                classes = [c for c in current['class'] if c]
                if classes:
                    path_parts.append(f"{current.name}.{classes[0]}")
                else:
                    path_parts.append(current.name)
            else:
                path_parts.append(current.name)
            current = current.parent
            if len(path_parts) > 5:  # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
                break
        
        selectors['hierarchical_path'] = ' > '.join(reversed(path_parts))
        
        return selectors

    def analyze_element_context(self, element: bs4_element.Tag) -> dict:
        """ÙŠØ­Ù„Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­ÙŠØ· Ø¨Ø§Ù„Ø¹Ù†ØµØ± Ù„ÙÙ‡Ù… Ø£ÙØ¶Ù„"""
        context = {}
        
        # Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
        if element.parent and element.parent.name:
            context['parent_tag'] = element.parent.name
            if element.parent.has_attr('class'):
                context['parent_classes'] = element.parent['class']
        
        # Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø´Ù‚ÙŠÙ‚Ø©
        siblings = element.find_next_siblings()[:3]  # Ø£ÙˆÙ„ 3 Ø¹Ù†Ø§ØµØ± Ø´Ù‚ÙŠÙ‚Ø©
        context['next_siblings'] = [sib.name for sib in siblings if sib.name]
        
        # Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­ÙŠØ·
        prev_text = ""
        next_text = ""
        if element.previous_sibling:
            prev_text = str(element.previous_sibling).strip()[:100]
        if element.next_sibling:
            next_text = str(element.next_sibling).strip()[:100]
            
        context['surrounding_text'] = {
            'before': prev_text,
            'after': next_text
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© (header, main, footer, nav, etc.)
        region_parent = element.find_parent(['header', 'main', 'footer', 'nav', 'aside', 'section', 'article'])
        if region_parent:
            context['page_region'] = region_parent.name
        
        return context

    def extract_semantic_info(self, element: bs4_element.Tag) -> dict:
        """ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ù†ØµØ±"""
        semantic = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù†Ù‰
        text = element.get_text(strip=True)
        if text:
            semantic['text_content'] = text
            semantic['text_length'] = len(text)
            
            # ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù†Øµ
            if re.search(r'\b(ØªØ³Ø¬ÙŠÙ„ Ø¯Ø®ÙˆÙ„|login|sign in)\b', text.lower()):
                semantic['text_type'] = 'login'
            elif re.search(r'\b(Ø¨Ø­Ø«|search)\b', text.lower()):
                semantic['text_type'] = 'search'
            elif re.search(r'\b(Ø¥Ø±Ø³Ø§Ù„|submit|send)\b', text.lower()):
                semantic['text_type'] = 'submit'
            elif re.search(r'\b(Ø¥Ù„ØºØ§Ø¡|cancel|close)\b', text.lower()):
                semantic['text_type'] = 'cancel'
            elif re.search(r'^\d+$', text):
                semantic['text_type'] = 'numeric'
            elif re.search(r'@.*\.(com|org|net)', text):
                semantic['text_type'] = 'email'
            elif text.startswith('http'):
                semantic['text_type'] = 'url'
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
        if element.has_attr('placeholder'):
            semantic['placeholder'] = element['placeholder']
        
        if element.has_attr('title'):
            semantic['title'] = element['title']
        
        if element.has_attr('alt'):
            semantic['alt_text'] = element['alt']
            
        if element.has_attr('aria-label'):
            semantic['aria_label'] = element['aria-label']
        
        if element.has_attr('role'):
            semantic['role'] = element['role']
        
        return semantic

    def categorize_element_advanced(self, element: bs4_element.Tag) -> dict:
        """ÙŠØµÙ†Ù Ø§Ù„Ø¹Ù†ØµØ± Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ©"""
        tag_name = element.name.lower()
        category = {}
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        if tag_name in self.MEDIA_TAGS:
            category['primary_type'] = 'media'
            category['media_subtype'] = tag_name
            if element.has_attr('src'):
                category['source_url'] = element['src']
        
        elif tag_name in self.INTERACTIVE_TAGS or element.has_attr('onclick'):
            category['primary_type'] = 'interactive'
            category['interactive_subtype'] = tag_name
            
            # ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
            if tag_name == 'input':
                input_type = element.get('type', 'text')
                category['input_type'] = input_type
                category['input_type_ar'] = self.INPUT_TYPES.get(input_type, input_type)
            
            elif tag_name == 'a':
                if element.has_attr('href'):
                    category['link_url'] = element['href']
                    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·
                    href = element['href']
                    if href.startswith('#'):
                        category['link_type'] = 'internal_anchor'
                    elif href.startswith('mailto:'):
                        category['link_type'] = 'email'
                    elif href.startswith('tel:'):
                        category['link_type'] = 'phone'
                    elif 'javascript:' in href:
                        category['link_type'] = 'javascript'
                    else:
                        category['link_type'] = 'external' if 'http' in href else 'internal'
        
        elif tag_name in self.TEXT_TAGS:
            category['primary_type'] = 'text'
            category['text_subtype'] = tag_name
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                category['heading_level'] = int(tag_name[1])
                category['importance'] = 'high'
            elif tag_name == 'p':
                category['importance'] = 'medium'
            else:
                category['importance'] = 'low'
        
        elif tag_name in self.NAVIGATION_TAGS:
            category['primary_type'] = 'navigation'
            category['nav_subtype'] = tag_name
        
        elif tag_name in self.LIST_TAGS:
            category['primary_type'] = 'list'
            category['list_subtype'] = tag_name
            # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
            list_items = element.find_all('li')
            category['list_items_count'] = len(list_items)
        
        else:
            category['primary_type'] = 'container'
            category['container_subtype'] = tag_name
        
        return category

    def extract_page_structure(self, soup: BeautifulSoup) -> dict:
        """ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØµÙØ­Ø©"""
        structure = {
            'page_title': soup.title.string if soup.title else None,
            'meta_description': None,
            'sections': defaultdict(int),
            'forms_count': len(soup.find_all('form')),
            'tables_count': len(soup.find_all('table')),
            'images_count': len(soup.find_all('img')),
            'links_count': len(soup.find_all('a')),
            'headings_hierarchy': defaultdict(int)
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØµÙ Ø§Ù„ØµÙØ­Ø©
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            structure['meta_description'] = meta_desc.get('content')
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        for i in range(1, 7):
            headings = soup.find_all(f'h{i}')
            structure['headings_hierarchy'][f'h{i}'] = len(headings)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_sections = ['header', 'nav', 'main', 'aside', 'footer', 'section', 'article']
        for section in main_sections:
            structure['sections'][section] = len(soup.find_all(section))
        
        return structure

    def analyze_page_content(self, url: str) -> dict:
        """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©"""
        
        with sync_playwright() as p:
            try:
                browser = p.chromium.connect_over_cdp("http://localhost:9222")
                context = browser.contexts[0]
                
                page = context.new_page()
                page.goto(url, wait_until='networkidle', timeout=90000)
                
                time.sleep(2)

                # Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                page_content = page.content()
                soup = BeautifulSoup(page_content, 'lxml')
                all_tags = soup.find_all(True)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©
                page_structure = self.extract_page_structure(soup)
                
                # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
                csv_data = []
                detailed_elements = []
                summary_stats = {
                    'total_elements': len(all_tags),
                    'by_category': defaultdict(int),
                    'interactive_elements': 0,
                    'media_elements': 0,
                    'text_elements': 0,
                    'high_priority_elements': 0
                }
                
                # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø¹Ù†ØµØ±
                for i, element in enumerate(all_tags, 1):
                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                    selectors = self.generate_advanced_selector(element)
                    context = self.analyze_element_context(element)
                    semantic = self.extract_semantic_info(element)
                    category = self.categorize_element_advanced(element)
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    primary_type = category.get('primary_type', 'unknown')
                    summary_stats['by_category'][primary_type] += 1
                    
                    if primary_type == 'interactive':
                        summary_stats['interactive_elements'] += 1
                    elif primary_type == 'media':
                        summary_stats['media_elements'] += 1
                    elif primary_type == 'text':
                        summary_stats['text_elements'] += 1
                    
                    if selectors.get('priority') == 'high':
                        summary_stats['high_priority_elements'] += 1
                    
                    # Ø¨ÙŠØ§Ù†Ø§Øª CSV Ø§Ù„Ù…Ø­Ø³Ù†Ø©
                    csv_row = {
                        'element_id': i,
                        'tag_name': element.name,
                        'primary_selector': selectors.get('id_selector') or selectors.get('class_selector') or selectors.get('tag_selector'),
                        'all_selectors': json.dumps(selectors, ensure_ascii=False),
                        'category': primary_type,
                        'subcategory': category.get(f'{primary_type}_subtype', ''),
                        'text_content': semantic.get('text_content', ''),
                        'semantic_type': semantic.get('text_type', ''),
                        'importance': category.get('importance', 'normal'),
                        'page_region': context.get('page_region', ''),
                        'is_interactive': primary_type == 'interactive',
                        'has_text': bool(semantic.get('text_content')),
                        'element_attributes': json.dumps(element.attrs, ensure_ascii=False)
                    }
                    csv_data.append(csv_row)
                    
                    # Ø¨ÙŠØ§Ù†Ø§Øª JSON Ø§Ù„Ù…ÙØµÙ„Ø© (ÙÙ‚Ø· Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù‡Ù…Ø©)
                    if (primary_type in ['interactive', 'media'] or 
                        category.get('importance') == 'high' or
                        semantic.get('text_content', '') and len(semantic['text_content']) > 10):
                        
                        detailed_element = {
                            'element_id': i,
                            'selectors': selectors,
                            'category': category,
                            'semantic_info': semantic,
                            'element_context': context,
                            'raw_attributes': element.attrs
                        }
                        detailed_elements.append(detailed_element)

                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
                result = {
                    "analysis_metadata": {
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "analyzer_version": "2.0",
                        "purpose": "Enhanced data for AI model analysis",
                        "analyzed_url": url,
                        "page_title": page.title()
                    },
                    "page_structure": dict(page_structure),
                    "elements_summary": dict(summary_stats),
                    "detailed_elements": detailed_elements,
                    "csv_data": csv_data,
                    "ai_assistant_guide": {
                        "element_identification": "Ø§Ø³ØªØ®Ø¯Ù… 'id_selector' Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… 'class_selector' Ù„Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ù‚ÙŠÙ‚",
                        "interaction_priority": "Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø°Ø§Øª 'priority': 'high' Ù„Ù‡Ø§ Ø£ÙˆÙ„ÙˆÙŠØ© ÙÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„",
                        "context_usage": "Ø§Ø³ØªØ®Ø¯Ù… 'element_context' Ùˆ 'semantic_info' Ù„ÙÙ‡Ù… Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ø¹Ù†ØµØ±",
                        "text_content_guidance": "Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ 'importance': 'high' Ù‡ÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"
                    }
                }
                
                return {"success": True, "data": result}

            except PlaywrightError as e:
                return {"success": False, "error": f"Playwright Error: {str(e)}"}
            except Exception as e:
                return {"success": False, "error": f"Unexpected Error: {str(e)}"}
            finally:
                if 'browser' in locals() and browser.is_connected():
                    browser.close()

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…Ø­Ù„Ù„
analyzer = WebPageAnalyzer()

# Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
UPLOAD_FOLDER = 'temp_files'
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

# === API Endpoints ===

@app.route('/', methods=['GET'])
def home():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù€ API"""
    return jsonify({
        "message": "ğŸ”¬ Ù…Ø­Ù„Ù„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - API",
        "version": "2.0",
        "endpoints": {
            "/": "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ù€ API",
            "/analyze": "ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© ÙˆÙŠØ¨ (POST)",
            "/health": "ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø©",
            "/download/<file_type>/<session_id>": "ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"
        },
        "usage": {
            "method": "POST",
            "endpoint": "/analyze",
            "body": {"url": "https://example.com"},
            "requirements": "Chrome browser with --remote-debugging-port=9222"
        }
    })

@app.route('/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø©"""
    try:
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Chrome
        with sync_playwright() as p:
            browser = p.chromium.connect_over_cdp("http://localhost:9222")
            browser.close()
            chrome_status = "Ù…ØªØµÙ„"
    except:
        chrome_status = "ØºÙŠØ± Ù…ØªØµÙ„"
    
    return jsonify({
        "status": "running",
        "chrome_connection": chrome_status,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    })

@app.route('/analyze', methods=['POST'])
def analyze_webpage():
    """ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© ÙˆÙŠØ¨"""
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø³Ù„Ø©
    if not request.is_json:
        return jsonify({"success": False, "error": "ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© JSON"}), 400
    
    data = request.get_json()
    
    if 'url' not in data:
        return jsonify({"success": False, "error": "ÙŠØ¬Ø¨ ØªÙ‚Ø¯ÙŠÙ… Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© (url)"}), 400
    
    url = data['url'].strip()
    
    if not (url.startswith("http://") or url.startswith("https://")):
        return jsonify({"success": False, "error": "Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http:// Ø£Ùˆ https://"}), 400
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    export_files = data.get('export_files', True)  # Ù‡Ù„ ØªØ±ÙŠØ¯ ØªØµØ¯ÙŠØ± Ù…Ù„ÙØ§Øª CSV Ùˆ JSON
    
    try:
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
        result = analyzer.analyze_page_content(url)
        
        if not result['success']:
            return jsonify(result), 500
        
        analysis_data = result['data']
        session_id = str(uuid.uuid4())[:8]  # Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„Ù„Ø¬Ù„Ø³Ø©
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
        if export_files:
            # Ø­ÙØ¸ CSV
            csv_filename = os.path.join(UPLOAD_FOLDER, f"analysis_{session_id}.csv")
            df = pd.DataFrame(analysis_data['csv_data'])
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            # Ø­ÙØ¸ JSON
            json_filename = os.path.join(UPLOAD_FOLDER, f"analysis_{session_id}.json")
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            analysis_data['download_links'] = {
                'csv': f"/download/csv/{session_id}",
                'json': f"/download/json/{session_id}"
            }
            analysis_data['session_id'] = session_id
        
        # Ø¥Ø²Ø§Ù„Ø© csv_data Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        if 'csv_data' in analysis_data:
            del analysis_data['csv_data']
        
        return jsonify({
            "success": True,
            "session_id": session_id,
            "message": f"ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­. ØªÙ… ØªØ­Ù„ÙŠÙ„ {analysis_data['elements_summary']['total_elements']} Ø¹Ù†ØµØ±",
            "data": analysis_data
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}"}), 500

@app.route('/download/<file_type>/<session_id>', methods=['GET'])
def download_file(file_type, session_id):
    """ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    
    if file_type not in ['csv', 'json']:
        return jsonify({"error": "Ù†ÙˆØ¹ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ø³ØªØ®Ø¯Ù… csv Ø£Ùˆ json"}), 400
    
    filename = f"analysis_{session_id}.{file_type}"
    filepath = os.path.join(UPLOAD_FOLDER, filename)
    
    if not os.path.exists(filepath):
        return jsonify({"error": "Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØªÙ‡"}), 404
    
    return send_file(filepath, as_attachment=True, download_name=filename)

@app.route('/analyze/quick', methods=['POST'])
def quick_analyze():
    """ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ - ÙŠÙØ±Ø¬Ø¹ ÙÙ‚Ø· Ø§Ù„Ù…Ù„Ø®Øµ ÙˆØ§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù‡Ù…Ø©"""
    
    if not request.is_json:
        return jsonify({"success": False, "error": "ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© JSON"}), 400
    
    data = request.get_json()
    
    if 'url' not in data:
        return jsonify({"success": False, "error": "ÙŠØ¬Ø¨ ØªÙ‚Ø¯ÙŠÙ… Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© (url)"}), 400
    
    url = data['url'].strip()
    
    try:
        result = analyzer.analyze_page_content(url)
        
        if not result['success']:
            return jsonify(result), 500
        
        analysis_data = result['data']
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ø¨Ø³Ø·Ø©
        quick_response = {
            "analysis_metadata": analysis_data['analysis_metadata'],
            "page_structure": {
                "page_title": analysis_data['page_structure']['page_title'],
                "forms_count": analysis_data['page_structure']['forms_count'],
                "images_count": analysis_data['page_structure']['images_count'],
                "links_count": analysis_data['page_structure']['links_count']
            },
            "elements_summary": analysis_data['elements_summary'],
            "interactive_elements": [
                elem for elem in analysis_data['detailed_elements']
                if elem['category']['primary_type'] == 'interactive'
            ][:10],  # Ø£ÙˆÙ„ 10 Ø¹Ù†Ø§ØµØ± ØªÙØ§Ø¹Ù„ÙŠØ©
            "important_headings": [
                elem for elem in analysis_data['detailed_elements']
                if elem['category'].get('importance') == 'high'
            ][:5]  # Ø£ÙˆÙ„ 5 Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ù‡Ù…Ø©
        }
        
        return jsonify({
            "success": True,
            "message": "ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¨Ù†Ø¬Ø§Ø­",
            "data": quick_response
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}"}), 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({"error": "Ø§Ù„ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©"}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({"error": "Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù…"}), 500

if __name__ == '__main__':
    print("=" * 70)
    print("ğŸ”¬ Ù…Ø­Ù„Ù„ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Flask API")
    print("=" * 70)
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…...")
    print("ğŸ“¡ API endpoints:")
    print("   GET  /                     - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† API")
    print("   GET  /health               - ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø©")
    print("   POST /analyze              - ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© ÙˆÙŠØ¨ ÙƒØ§Ù…Ù„")
    print("   POST /analyze/quick        - ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹")
    print("   GET  /download/<type>/<id> - ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª")
    print()
    print("--- âš ï¸ ØªØ°ÙƒÙŠØ± Ù…Ù‡Ù… ---")
    print("ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ Google Chrome Ù…Ø¹:")
    print("chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-debug")
    print("=" * 70)
    
    app.run(host='0.0.0.0', port=5000, debug=True)